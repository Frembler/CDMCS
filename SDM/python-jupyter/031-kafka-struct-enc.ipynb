{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka structured messages and avro encoding\n",
    "\n",
    "A common use-case is to send messages payloads into kafka in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"json\"\n",
    "server = \"kafka:9092\"\n",
    "producer = KafkaProducer(bootstrap_servers=server, value_serializer=lambda v: json.dumps(v).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load some data. Alas, the example apache log file is in unstructured common format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233.167.247.91 - - [26/Sep/2017:07:54:24 +0000] \"POST /app/main/posts HTTP/1.0\" 404 4948 \"http://www.patel.org/post.asp\" \"Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_6_4) AppleWebKit/5360 (KHTML, like Gecko) Chrome/14.0.812.0 Safari/5360\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src = \"/home/jovyan/data/SDM/logs/apache-short.log\"\n",
    "with open(src, \"r\") as logfile:\n",
    "    print(logfile.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I will need to parse this file into structured JSON. I will write a simple grok-ish parser for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "grokpattern = \"((?:\\d{1,3}\\.){3}\\d{1,3}) - (\\S+) \\[(.+)\\] \\\"(.+?)\\\" (\\d+) (\\d+) \\\"(.+?)\\\" \\\"(.+?)\\\"\"\n",
    "grokpattern = re.compile(grokpattern)\n",
    "\n",
    "grokData = []\n",
    "\n",
    "with open(src, \"r\") as logfile:\n",
    "    i = 0\n",
    "    for line in logfile:\n",
    "        i += 1\n",
    "        vars = grokpattern.match(line)\n",
    "        ip = vars.group(1)\n",
    "        struct = {\n",
    "            \"ipv4\": vars.group(1),\n",
    "            \"user\": vars.group(2),\n",
    "            \"timestamp\": vars.group(3),\n",
    "            \"request\": vars.group(4),\n",
    "            \"response\": int(vars.group(5)),\n",
    "            \"bytes\": int(vars.group(6)),\n",
    "            \"referer\": vars.group(7),\n",
    "            \"ua\": vars.group(8)\n",
    "        }\n",
    "        grokData.append(struct)\n",
    "        producer.send(topic, struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConsumerRecord(topic='json', partition=1, offset=5, timestamp=1519578223633, timestamp_type=0, key=None, value=b'{\"ipv4\": \"66.76.195.231\", \"user\": \"-\", \"timestamp\": \"26/Sep/2017:07:54:24 +0000\", \"request\": \"GET /wp-admin HTTP/1.0\", \"response\": 200, \"bytes\": 4994, \"referer\": \"http://bolton-ali.biz/\", \"ua\": \"Mozilla/5.0 (compatible; MSIE 5.0; Windows NT 5.0; Trident/5.1)\"}', checksum=-252235342, serialized_key_size=-1, serialized_value_size=260)\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "consumer = KafkaConsumer(topic, group_id=None, bootstrap_servers=server, auto_offset_reset='earliest')\n",
    "\n",
    "i = 0\n",
    "for msg in consumer:\n",
    "    if i == 5:\n",
    "        print(msg)\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break\n",
    "consumer.close(autocommit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Kafka stores raw bytes, it it's essentially agnostic to message source format. Storing such clearly sturctural data as bytearray from string is quite inefficient. For example, if we wanted to stora netflow, then kafka disk usage can fill quite fast. \n",
    "\n",
    "Avro is a popular bigdata encoding format that can be used to mitigate this issue. \n",
    "\n",
    "* https://avro.apache.org/docs/1.8.2/spec.html\n",
    "\n",
    "Firstly, we will need to specify our message schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "apacheSchema = {\n",
    "    \"name\": \"apache\",\n",
    "    \"type\": \"record\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ipv4\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"user\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"timestamp\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"request\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"response\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"bytes\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"referer\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ua\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro.schema, avro.io\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = avro.schema.Parse(json.dumps(apacheSchema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a new producer to another topic. Kafka will not stop you from sending mixed encoded and plain data into same topic, but it will make your like very difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"binary\"\n",
    "server = \"kafka:9092\"\n",
    "producer = KafkaProducer(bootstrap_servers=server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode data and send it to kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in grokData:\n",
    "    writer = avro.io.DatumWriter(schema)\n",
    "    byteWriter = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(byteWriter)\n",
    "    writer.write(log, encoder)\n",
    "    raw = byteWriter.getvalue()\n",
    "    producer.send(topic, raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConsumerRecord(topic='binary', partition=2, offset=0, timestamp=1519579033146, timestamp_type=0, key=None, value=b'\\x18117.18.66.28\\x02-426/Sep/2017:07:54:24 +0000*GET /explore HTTP/1.0\\x90\\x03\\xfeL6http://www.nguyen-king.com/\\xa6\\x01Mozilla/5.0 (X11; Linux x86_64; rv:1.9.7.20) Gecko/2016-10-25 04:14:06 Firefox/10.0', checksum=-1814158432, serialized_key_size=-1, serialized_value_size=181)\n"
     ]
    }
   ],
   "source": [
    "consumer = KafkaConsumer(topic, group_id=None, bootstrap_servers=server, auto_offset_reset='earliest')\n",
    "\n",
    "i = 0\n",
    "for msg in consumer:\n",
    "    if i % 10 == 0:\n",
    "        print(msg)\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break\n",
    "consumer.close(autocommit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now byte data when attempting to consume the topic. We are also missing JSON keys as our schema omits the need for those. To get the data out in raw format, we need to reverse the avro process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = avro.io.DatumReader(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ipv4': '85.83.140.71', 'user': '-', 'timestamp': '26/Sep/2017:07:54:24 +0000', 'request': 'GET /search/tag/list HTTP/1.0', 'response': 200, 'bytes': 4939, 'referer': 'http://www.cole.com/', 'ua': 'Mozilla/5.0 (Windows 95) AppleWebKit/5310 (KHTML, like Gecko) Chrome/15.0.806.0 Safari/5310'}\n"
     ]
    }
   ],
   "source": [
    "consumer = KafkaConsumer(topic, group_id=None, bootstrap_servers=server, auto_offset_reset='earliest')\n",
    "\n",
    "i = 0\n",
    "for msg in consumer:\n",
    "    i += 1\n",
    "    bytes_reader = io.BytesIO(msg.value)\n",
    "    decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "    payload = reader.read(decoder)\n",
    "    if i == 10:\n",
    "        print(payload)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
